# Artifact for OOPSLA'18 Submission 'Scopes as Types'

## Getting Started

> Getting Started Guide
>
> The Getting Started Guide should contain setup instructions
> (including, for example, a pointer to the VM player software, its
> version, passwords if needed, etc.) and basic testing of your
> artifact that you expect a reviewer to be able to complete in 30
> minutes. Reviewers will follow all the steps in the guide during an
> initial kick-the-tires phase. The Getting Started Guide should be as
> simple as possible, and yet it should stress the key elements of
> your artifact. Anyone who has followed the Getting Started Guide
> should have no technical difficulties with the rest of your
> artifact.
>
> Where appropriate, include descriptions of and links to files
> (included in the archive) that represent expected outputs (e.g., the
> log files expected to be generated by your tool on the given
> inputs); if there are warnings that are safe to be ignored, explain
> which ones they are.

This section explains how to get the artifact running and what can be
inspected. The artifact consists of a Spoofax Eclipse workspace with
language implementation projects for the case studies presented in the
paper. The static semantics of these languages are defined in Statix,
the constraint language defined in the paper. First we explain how to
get Spoofax, and build the language projects. Then we explain the
structure of the language projects, and what can be inspected and
tested.

### Getting Spoofax and Building the Language Projects

The case studies are implemented in the Spoofax language workbench,
which supports Statix is one of its meta-languages.

- Download the appropriate Spoofax archive for you platform from
  https://github.com/MetaBorgCube/oopsla18-artifact/releases/ or
  https://buildfarm.metaborg.org/job/metaborg/job/spoofax-releng/job/master/659/artifact/dist/spoofax/eclipse/.

- Install and run Spoofax by unpacking the archive, and starting
  `eclipse` or `eclipse.exe`. When Spoofax is started, it will ask for
  a workspace; select the artifact root directory. Workspaces can be
  changed using the `File > Switch Workspace` menu, if necessary.
  
- Build all language projects using the `Project > Build All`
  menu. After a successful language build, the console shows something
  like:
  
      Reloading language project eclipse:///lang.sysf

### Inspecting the Language Projects

The workspace contains three language projects, for the three case
studies: the Simply Typed Lambda Calculus with Structural Records in
`lang.stlcrec`, System F in `lang.sysf`, and Featherweight Generic
Java in `lang.fgj`. Each project contains a Statix specification of
the static semantics, some example programs, and a test suite.

- The Statix specification of a language is found in the file
  `trans/statics.stx`. The file can be opened in an editor by
  double-clicking.

- Example programs can be found in the `example` folder of a language
  project. Double-clicking opens an editor and starts type checking
  for the file, using the language's Statix specification. If the file
  contains errors, a red marker appears at the top of the
  editor. Depending on the size of the program, type checking can take
  some time. Only after type checking is done will the error marker be
  updated. When done, the console shows a status line similar to:

      Solved 415 constraints (103 delays) with 0 failed and 0 remaining constraint(s).

  If the number of failed or remaining constraints is not zero, the
  program did not successfully type check.

- Every language projects includes a test suite for the static
  semantics of the language, located in the `test` directory. Test
  files have a `.spt` extension, and can be opened and inspected in an
  editor by double-clicking. Failing tests are marked with a red
  marker. If a file contains many tests, it may take a while before
  the tests are finished and the success and failures are correctly
  marked. Use the console or the progress window to check if the tests
  are still running.

- All tests in a directory can be run by selected the directory in the
  `Package Explorer`, and selecting the `Spoofax (meta) > Run all
  selected tests` menu. A test runner will appear listing all tests,
  showing progress and which tests success or failure. Running all
  tests (especially for FGJ) takes quite a while. For faster results,
  open individual files, or run fewer tests by selecting
  subdirectories of the `test` directory.

## Case study languages

### General remarks:

> The artifactâ€™s documentation should include the following:
> 
> * A list of claims from the paper supported by the artifact, and how/why.
> * A list of claims from the paper not supported by the artifact, and how/why.
>
> Example: Performance claims cannot be reproduced in VM, authors are
> not allowed to redistribute specific benchmarks, etc. Artifact
> reviewers can then center their reviews / evaluation around these
> specific claims.

- Focus is on expressivity and correctness

- Not on performance

- Not on good error messages

### STLC+Rec

- STLC, let-expressions

- Records with structural subtyping

- Record extension

- Type let binders

### System F

- STLC, let-expressions

- Type binders (`Fun`)

- Type let binders

### FGJ

- Classes

- Generics

Differences from FGJ:

- Constructor arguments do not have to match fields.

- Fields can be initialized with arbitrary expressions, instead of just variable references.

- No check that every field is initialized.

## Step by Step Instructions

> Step-by-Step Instructions for how you propose to evaluate your
> artifact (with appropriate connections to the relevant sections of
> your paper);
>
> The Step by Step Instructions explain how to reproduce any
> experiments or other activities that support the conclusions in your
> paper. Write this for readers who have a deep interest in your work
> and are studying it to improve it or compare against it. If your
> artifact runs for more than a few minutes, point this out and
> explain how to run it on smaller inputs.
>
> Where appropriate, include descriptions of and links to files
> (included in the archive) that represent expected outputs (e.g., the
> log files expected to be generated by your tool on the given
> inputs); if there are warnings that are safe to be ignored, explain
> which ones they are.

