# Case study languages

## General remarks:

- Focus is on expressivity and correctness

- Not on performance

- Not on good error messages

## STLC+Rec

- STLC, let-expressions

- Records with structural subtyping

- Record extension

- Type let binders

## System F

- STLC, let-expressions

- Type binders (`Fun`)

- Type let binders

## FGJ

- Classes

- Generics

Differences from FGJ:

- Constructor arguments do not have to match fields.

- Fields can be initialized with arbitrary expressions, instead of just variable references.

- No check that every field is initialized.

# Getting Started

- Find the Spoofax appropriate for your platform in the `spoofax` directory.

- Start it and select the `workspace` from the artifact as the workspace.

- Make sure `??? > Build automatically` is checked.

- Do a clean build of all languages in the project using `??? > Clean`

- Build all languages using `??? > Build all`.

- Files are analyzed when they are opened in the editor or when edited. If analysis performance is taking too much time, consider checking `Spoofax (meta) > Disable editor analysis`. Note that error messages in the editors may be wrong if this checked. If unchecked, modify the file to trigger re-analysis again.

# Step-by-Step Evaluation

## Inspecting the Statix specs

Specs for each language are found as `trans/statics.stx`.

## Running the language test suites

Select the `test/` directory in the project explorer on the left, and select `Spoofax (meta) > Run all selected tests` from the menu. This will start a test runner. Running all tests may take a while. Jump to tests by double-clicking in the test runner overview.

## Writing example programs

All projects contain an `example` folder with some example programs. Inspect and modify these programs. If the program has errors, a red marker appears at the top of the file. If the program is correct, no marker appears.




# From https://2018.splashcon.org/track/splash-2018-OOPSLA-Artifacts:


## Overview of the Artifact

Your overview should consist of two parts:

    a Getting Started Guide and
    Step-by-Step Instructions for how you propose to evaluate your artifact (with appropriate connections to the relevant sections of your paper);

The Getting Started Guide should contain setup instructions (including, for example, a pointer to the VM player software, its version, passwords if needed, etc.) and basic testing of your artifact that you expect a reviewer to be able to complete in 30 minutes. Reviewers will follow all the steps in the guide during an initial kick-the-tires phase. The Getting Started Guide should be as simple as possible, and yet it should stress the key elements of your artifact. Anyone who has followed the Getting Started Guide should have no technical difficulties with the rest of your artifact.

The Step by Step Instructions explain how to reproduce any experiments or other activities that support the conclusions in your paper. Write this for readers who have a deep interest in your work and are studying it to improve it or compare against it. If your artifact runs for more than a few minutes, point this out and explain how to run it on smaller inputs.

Where appropriate, include descriptions of and links to files (included in the archive) that represent expected outputs (e.g., the log files expected to be generated by your tool on the given inputs); if there are warnings that are safe to be ignored, explain which ones they are.

The artifactâ€™s documentation should include the following:

    A list of claims from the paper supported by the artifact, and how/why.
    A list of claims from the paper not supported by the artifact, and how/why.

Example: Performance claims cannot be reproduced in VM, authors are not allowed to redistribute specific benchmarks, etc. Artifact reviewers can then center their reviews / evaluation around these specific claims.
